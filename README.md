# Implement
1. build_vocab에서 먼저 vocab 생성

python build_vocab.py

2. main 실행
pythom main.py --epochs ....

# Model 

![transformer](https://user-images.githubusercontent.com/76771847/124423907-08ad9980-dda1-11eb-9ed9-4d9872c7131f.png)

![mha_img_original](https://user-images.githubusercontent.com/76771847/124432128-39470080-ddac-11eb-918e-062fcab418d4.png)

![Details-of-multi-head-attention-building-blocks](https://user-images.githubusercontent.com/76771847/124434814-49141400-ddaf-11eb-9fed-b543f423c183.png)

# Paper

https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf


## Reference

https://www.youtube.com/watch?v=AA621UofTUA&t=960s

https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/

## Code

official: https://github.com/tensorflow/tensor2tensor

https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.py

https://github.com/Huffon/pytorch-transformer-kor-eng

## Result

![Screenshot from 2021-07-07 00-01-15](https://user-images.githubusercontent.com/76771847/124622923-7b5c6900-deb6-11eb-803d-54320aeacdb9.png)

![Screenshot from 2021-07-06 23-59-33](https://user-images.githubusercontent.com/76771847/124622928-7c8d9600-deb6-11eb-88ae-f5533a040afb.png)

