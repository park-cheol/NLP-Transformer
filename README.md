# Model 

![transformer](https://user-images.githubusercontent.com/76771847/124423907-08ad9980-dda1-11eb-9ed9-4d9872c7131f.png)

![mha_img_original](https://user-images.githubusercontent.com/76771847/124432128-39470080-ddac-11eb-918e-062fcab418d4.png)

![Details-of-multi-head-attention-building-blocks](https://user-images.githubusercontent.com/76771847/124434814-49141400-ddaf-11eb-9fed-b543f423c183.png)

# Paper

https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf


## Reference

https://www.youtube.com/watch?v=AA621UofTUA&t=960s

https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/

## Code

official: https://github.com/tensorflow/tensor2tensor

https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.py

https://github.com/Huffon/pytorch-transformer-kor-eng

## 기법
Opimizer: 
